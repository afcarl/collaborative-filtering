\documentclass[10pt,twocolumn]{article}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{geometry}
\usepackage{amsmath}


\DeclareMathOperator*{\argmin}{arg\,min}

\title{TC1 Project: Know your customers}

\author{Aris Tritas \and Laurent Cetinsoy}

\begin{document}

\maketitle

\begin{abstract}

The objective of this analysis is to predict a rating that would be assigned by customers to items (which could be e.g. a firm's services). 
The problem was designed as a case of sequential prediction. Moreover, it can be viewed as a collaborating filtering. It is solved using the following methods: firstly, an Non-negative Matrix Factorization is applied. Then,  and autoencoders.

\end{abstract}

\section{Introduction}

In collaborative filtering, each item $i$ of a collection is rated by one or more members of a population $n_u$. The aim is to predict the rating that a person would give to an unseen item, so as to be able to recommend new items to users such that it fits their preferences with high probability.

In this setup, let $R$ represent a $n \times m$ matrix. The rating of the $j^{th}$ item from the $i^{th}$ user is $r_{ij}$.

\subsection{Data}
We are given a training set describing the ratings assigned by customers to items in some given order.
Dataset size: 2536705 examples in the training set, 1306637 in the test set. Total number of users 93705 (of which only 92088 are in the training set) and 3561 items. The dataset is very sparse,  less than $1\% $ of the ratings matrix is filled.
Ratings distribution: [114214, 251596, 728518, 860463, 581913] In proportion of the total number of ratings: [0.0450, 0.0991, 0.2872, 0.3392, 0.2293].

\section{Hidden Markov Model}
It is possible to model the problem of sequential prediction using a hidden Markov model. In order to do that efficiently, it is assumed that 
\section{Non negative matrix factorization}

In non negative factorizations we seek SPD matrices U and V such that the rating product matrix can be factorized as $R = UV^T$. It is possible to find such matrices that minimize the reconstruction error defined below by using an iterative optimization method (e.g gradient descent): 

\begin{equation}
\begin{split}
\argmin_{U,V} & \frac{1}{2} ||R - UV||^2 \\
			  + & \alpha \lambda_1 (||U||_1 + ||V||_1) \\
			  + & \frac{1}{2} \alpha \lambda_2  (||U||^2 + ||V||^2) 
\end{split}
\end{equation}  

The reconstruction error is computed over all non negative coefficients of the R matrix. In other words, the algorithm only tries to recover the available ratings. 

\subsection{ALS-WR}

The Alternating Least Squares with Regularization algorithm \cite{zhou2008large}  minimizes \eqref{1}, which is non-convex as both $U$ and $V$ are unknown. It does so by alternatively updating U while keeping V fixed and then updating V while keeping U fixed. The parameters used were $\lambda = 0.01$ and a latent dimension $d = 100$.


\section{Auto-encoder}

The auto-encoder is a neural network approach used in unsupervised learning for dimensionality reduction. It optimizes the following loss function:

\begin{equation}
\begin{split}
L_{2, \alpha, \beta} = &\alpha ( \sum_{j \in \mathcal{C}(\tilde{x})} [ nn(\tilde{x}_j) - x_j ]^2 \\
+ & \beta ( \sum_{j \notin \mathcal{C}(\tilde{x})} [nn(\tilde{x}_j) - x_j ]^2)
\end{split}
\end{equation}
where $\mathcal{C}(\tilde{x})$ The first term is a loss over corrupted version of the inputs which the network seeks to denoise. The second term is the reconstruction loss.

In the supervised classification task, the auto-encoder's objective is to lower a prediction error over a training set.

The approach proposed by \cite{strub2016hybrid} was used to tackle our collaborating filter problem. Two different auto-encoders can be trained depending on whether the network seeks to make user or item vectors dense:
\begin{itemize}
\item U-CFN is defined as $\hat{\textbf{u}}_i = nn(\textbf{u}_i)$
\item V-CFN is defined as $\hat{\textbf{v}}_j = nn(\textbf{v}_j)$ 
\end{itemize}

\section{Temporal dynamics}
In the context of matrix factorization, an idea is to add temporal dynamics is to bias either sides of the matrix factors. In the context of the auto-encoder, there are two possibilities: either create multiple examples for each user incrementally augmented with entries from the rated items sequence, or add the sequence information as additional (summed or averaged) features \cite{youtube2016}.

\section{Implicit side information}
Another source of information the model can take into account is the fact that the test set provides entries indicating which items have further been rated. This information can be integrated following \cite{salakhutdinov2007restricted}.

\section{Results}
The metrics used to evaluate the performance of each training method for this type of problem is either the RMSE (root mean squared error) or the MAE (mean absolute error). The scores below represent the RMSE and were computed on a validation set selected uniformly at random amongst the training set:
\begin{tabular}{|l|c|c|r|}
  \hline
  NMF & ALS-WR & User AE & Item AE \\
  \hline
  3.70 & 0.934 & 0.913 & 0.938 \\
  \hline
\end{tabular}


\section{Conclusion}
Different methods were compared in order to solve the problem of customer affinity prediction. When sparsity becomes an issue, no single method works best: exploiting the problem structure and combining multiple approaches has the potential of achieving the best results.

\bibliographystyle{plain}
\bibliography{biblio} % mon fichier de base de donn√©es s'appelle bibli.bib


\end{document}